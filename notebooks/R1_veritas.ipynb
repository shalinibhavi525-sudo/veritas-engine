{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch scikit-learn evaluate accelerate -q\n",
        "\n",
        "print(\"âœ… Libraries Installed.\")\n"
      ],
      "metadata": {
        "id": "hrtZHprt2xbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers tokenizers optimum onnxruntime\n",
        "\n",
        "!pip install --upgrade pip -q\n",
        "!pip install transformers optimum[onnxruntime] datasets torch onnx -q\n",
        "\n",
        "print(\"âœ… Installation complete.\")"
      ],
      "metadata": {
        "id": "GQyyyLsAK8Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "import os\n",
        "\n",
        "!wget -q https://www.cs.ucsb.edu/~william/data/liar_dataset.zip\n",
        "!unzip -o -q liar_dataset.zip\n",
        "col_names = ['id', 'label_text', 'statement', 'subjects', 'speaker', 'job', 'state', 'party',\n",
        "             'barely_true_cts', 'false_cts', 'half_true_cts', 'mostly_true_cts', 'pants_fire_cts', 'context']\n",
        "print(\"Reading files...\")\n",
        "df_train = pd.read_csv('train.tsv', sep='\\t', header=None, names=col_names, on_bad_lines='skip', quoting=3)\n",
        "df_val = pd.read_csv('valid.tsv', sep='\\t', header=None, names=col_names, on_bad_lines='skip', quoting=3)\n",
        "\n",
        "label_map = {\n",
        "    'true': 1,\n",
        "    'mostly-true': 1,\n",
        "    'half-true': 1,\n",
        "    'barely-true': 0,\n",
        "    'false': 0,\n",
        "    'pants-fire': 0\n",
        "}\n",
        "\n",
        "df_train['label'] = df_train['label_text'].map(label_map)\n",
        "df_val['label'] = df_val['label_text'].map(label_map)\n",
        "\n",
        "df_train = df_train[['statement', 'label']].dropna()\n",
        "df_val = df_val[['statement', 'label']].dropna()\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(df_train),\n",
        "    'validation': Dataset.from_pandas(df_val)\n",
        "})\n",
        "\n",
        "print(\" Data Loaded Manually.\")\n",
        "print(f\"Training examples: {len(dataset['train'])}\")\n",
        "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
        "print(\"Example:\", dataset['train'][0])"
      ],
      "metadata": {
        "id": "tbXnPXbi7F2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "print(\"Tokenizing data\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Training started.\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "KV-J6Frq7IUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\" Force-moving model to {device.upper()} to fix error...\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Phase 1: Calculating Accuracy\")\n",
        "eval_results = trainer.evaluate()\n",
        "accuracy = eval_results[\"eval_accuracy\"]\n",
        "\n",
        "print(\"Phase 2: Measuring Speed\")\n",
        "model_cpu = model.to(\"cpu\")\n",
        "dummy_input = \"The unemployment rate has dropped significantly this year.\"\n",
        "\n",
        "inputs = tokenizer(dummy_input, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "for _ in range(10):\n",
        "    with torch.no_grad():\n",
        "        _ = model_cpu(**inputs)\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(50):\n",
        "    with torch.no_grad():\n",
        "        _ = model_cpu(**inputs)\n",
        "end_time = time.time()\n",
        "\n",
        "avg_latency = (end_time - start_time) / 50 * 1000\n",
        "\n",
        "torch.save(model.state_dict(), \"baseline_model.pt\")\n",
        "size_mb = os.path.getsize(\"baseline_model.pt\") / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"BASELINE RESULTS\")\n",
        "print(f\"Model Size:   {size_mb:.2f} MB  (Target: <50 MB)\")\n",
        "print(f\"Latency (CPU):{avg_latency:.2f} ms  (Target: <50 ms)\")\n",
        "print(f\"Accuracy:     {accuracy*100:.2f}%\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "27n-owpi-AUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.quantization\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "print(\"Starting Compression\")\n",
        "\n",
        "model_cpu = model.to(\"cpu\")\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_cpu,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "torch.save(quantized_model.state_dict(), \"quantized_model.pt\")\n",
        "q_size_mb = os.path.getsize(\"quantized_model.pt\") / (1024 * 1024)\n",
        "\n",
        "dummy_input = \"The unemployment rate has dropped significantly this year.\"\n",
        "inputs = tokenizer(dummy_input, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = quantized_model(**inputs)\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(100):\n",
        "    _ = quantized_model(**inputs)\n",
        "end_time = time.time()\n",
        "\n",
        "q_avg_latency = (end_time - start_time) / 100 * 1000\n",
        "\n",
        "print(\"Accuracy on Validation Set\")\n",
        "quantized_model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "val_dataset = tokenized_datasets[\"validation\"]\n",
        "\n",
        "for i in range(len(val_dataset)):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        input_ids = torch.tensor([val_dataset[i][\"input_ids\"]])\n",
        "        attention_mask = torch.tensor([val_dataset[i][\"attention_mask\"]])\n",
        "        label = val_dataset[i][\"label\"]\n",
        "\n",
        "\n",
        "        outputs = quantized_model(input_ids, attention_mask=attention_mask)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "        if prediction == label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Checked {i}/{len(val_dataset)}...\")\n",
        "\n",
        "q_accuracy = correct / total\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"COMPRESSION RESULTS\")\n",
        "print(f\"Old Size: {size_mb:.2f} MB  ->  ðŸ†• New Size: {q_size_mb:.2f} MB\")\n",
        "print(f\"Old Speed: {avg_latency:.2f} ms ->  ðŸ†• New Speed: {q_avg_latency:.2f} ms\")\n",
        "print(f\" Old Acc:  {accuracy*100:.2f}%   ->  ðŸ†• New Acc:   {q_accuracy*100:.2f}%\")\n",
        "print(f\"COMPRESSION RATIO: {size_mb/q_size_mb:.2f}x smaller\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "CD_-dCQHBUqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "\n",
        "save_directory = \"./veritas_trained_temp\"\n",
        "if os.path.exists(save_directory):\n",
        "    shutil.rmtree(save_directory)\n",
        "\n",
        "trainer.model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "model_onnx = ORTModelForSequenceClassification.from_pretrained(\n",
        "    save_directory,\n",
        "    export=True\n",
        ")\n",
        "model_onnx.save_pretrained(\"onnx_model_raw\")\n",
        "\n",
        "quantizer = ORTQuantizer.from_pretrained(model_onnx)\n",
        "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
        "\n",
        "quantizer.quantize(\n",
        "    save_dir=\"onnx_quantized\",\n",
        "    quantization_config=qconfig,\n",
        ")\n",
        "\n",
        "onnx_file = \"onnx_quantized/model_quantized.onnx\"\n",
        "final_size = os.path.getsize(onnx_file) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"SQUEEZE COMPLETE\")\n",
        "print(f\"Final Model Size: {final_size:.2f} MB\")\n",
        "print(f\"Target Status: {'SUCCESS (Under 100MB)' if final_size < 100 else 'FAILED'}\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "62G8AbQuojcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sizes = [255.45, 132.29, final_size]\n",
        "size_labels = ['Original FP32', 'PyTorch Quant', 'Veritas ONNX']\n",
        "size_colors = ['#ff9999', '#ffcc99', '#99ff99']\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(size_labels, sizes, color=size_colors, edgecolor='black')\n",
        "\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 3,\n",
        "             f'{bar.get_height():.1f} MB', ha='center', fontweight='bold')\n",
        "\n",
        "plt.axhline(y=100, color='r', linestyle='--', alpha=0.6)\n",
        "plt.text(2.1, 105, 'Browser Limit (100MB)', color='red', fontweight='bold')\n",
        "plt.title('Figure 1: Storage Footprint Reduction', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Size (MB)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Figure1.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MUBNGvfhthOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "final_model_path = \"onnx_quantized\"\n",
        "model = ORTModelForSequenceClassification.from_pretrained(final_model_path, file_name=\"model_quantized.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./veritas_trained_temp\")\n",
        "\n",
        "test_claims = [\n",
        "    \"The unemployment rate reached a 10-year low today.\",\n",
        "    \"Scientists found a hidden city on Mars inhabited by giants.\",\n",
        "    \"The cost of living has increased in major urban centers.\"\n",
        "]\n",
        "\n",
        "print(f\"\\n{'CLAIM':<60} | {'PREDICTION':<12} | {'CONF.'} | {'TIME'}\")\n",
        "print(\"-\" * 95)\n",
        "\n",
        "for claim in test_claims:\n",
        "\n",
        "    inputs = tokenizer(claim, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    latency = (time.time() - start) * 1000\n",
        "\n",
        "    raw_logits = outputs.logits\n",
        "\n",
        "    if isinstance(raw_logits, np.ndarray):\n",
        "        logits = torch.from_numpy(raw_logits)\n",
        "    else:\n",
        "        logits = raw_logits\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    conf, pred = torch.max(probs, dim=1)\n",
        "\n",
        "    label = \"RELIABLE\" if pred.item() == 1 else \"UNRELIABLE\"\n",
        "\n",
        "    print(f\"{claim[:58]:<60} | {label:<12} | {conf.item():.1%} | {latency:.1f}ms\")\n"
      ],
      "metadata": {
        "id": "tDBvoJOKPuzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}